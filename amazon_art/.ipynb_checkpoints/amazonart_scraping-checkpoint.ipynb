{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import pandas as pd\n",
    "import time\n",
    "import random\n",
    "import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def encodestring(var):\n",
    "    if var is not None:\n",
    "        return var.encode('utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def scrapePainting(url):\n",
    "    r = requests.get(url)\n",
    "    print r, url\n",
    "    b = BeautifulSoup(r.text, 'html.parser')\n",
    "    \n",
    "    url_id = r.url.split('/')[-1]\n",
    "    \n",
    "    artist = b.find('a', {'id':'fine-ART-ProductLabelArtistNameLink'})\n",
    "    if artist is not None:\n",
    "        artist = b.find('a', {'id':'fine-ART-ProductLabelArtistNameLink'}).text\n",
    "        \n",
    "    title = b.find('span', {'id': 'fineArtTitle'})\n",
    "    if title is not None:\n",
    "        title = b.find('span', {'id': 'fineArtTitle'}).text\n",
    "        \n",
    "    image = re.findall('http://ecx.images-amazon.com/images/I/.*\\.jpg', r.text)\n",
    "    if image is not None:\n",
    "        image = re.findall('http://ecx.images-amazon.com/images/I/.*\\.jpg', r.text)\n",
    "        \n",
    "    price = b.find('span', {'id': 'priceblock_ourprice'})\n",
    "    if price is not None:\n",
    "        price = b.find('span', {'id': 'priceblock_ourprice'}).text.replace('$','').replace(',', '')\n",
    "        if '.' in price:\n",
    "            price = float(price.replace('.', ''))/100\n",
    "        else:\n",
    "            price = float(price)\n",
    "            \n",
    "    desc = b.find('div', {'id': 'productDescription_feature_div'})\n",
    "    if desc is not None:\n",
    "        desc = b.find('div', {'id': 'productDescription_feature_div'}).getText().replace('\\n','')\n",
    "        \n",
    "    height = None\n",
    "    width = None\n",
    "    size = b.find('span', {'id': 'mnba_buybox_size'})\n",
    "    if size is not None:\n",
    "        size = [a.strip() for a in b.find('span', {'id': 'mnba_buybox_size'}).text.split('x')]\n",
    "        if len(size) == 2:\n",
    "            height = float(size[0].replace('in.','').replace('in','').strip())\n",
    "            width = float(size[1].replace('in.','').replace('in','').strip())\n",
    "            depth = 1.\n",
    "        elif len(size) == 3:\n",
    "            height = float(size[0].replace('in.','').replace('in','').strip())\n",
    "            width = float(size[1].replace('in.','').replace('in','').strip())\n",
    "            depth = float(size[2].replace('in.','').replace('in','').strip())\n",
    "    \n",
    "    size_variations = b.find('div', {'id': 'variation_size_name'})\n",
    "    if size_variations is not None:\n",
    "        size = [a.strip() for a in size_variations.find('span', {'class': \"a-size-base\"}).text.split('x')]\n",
    "        \n",
    "        if len(size) == 2:\n",
    "            height = float(size[0].replace('in.','').replace('in','').strip())\n",
    "            width = float(size[1].replace('in.','').replace('in','').strip())\n",
    "            depth = 1.\n",
    "        elif len(size) == 3:\n",
    "            height = float(size[0].replace('in.','').replace('in','').strip())\n",
    "            width = float(size[1].replace('in.','').replace('in','').strip())\n",
    "            depth = float(size[2].replace('in.','').replace('in','').strip())\n",
    "            \n",
    "        price = size_variations.find('span', {'class': \"a-size-mini\"}).text.replace('$','').replace(',', '')\n",
    "        if '.' in price:\n",
    "            price = float(price.replace('.', ''))/100\n",
    "        else:\n",
    "            price = float(price)\n",
    "    \n",
    "\n",
    "    return {'url_id': encodestring(url_id), 'artist': encodestring(artist), 'title':encodestring(title), 'image': image, 'price': price, \n",
    "            'description':encodestring(desc), 'height': height, 'width': width, 'size':size}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "scrapePainting('http://www.amazon.com/Walk-Away-21x21-in/dp/B0170E9ARC')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "paintings = []\n",
    "\n",
    "for page in range(1,219):\n",
    "    \n",
    "    page_url = 'http://www.amazon.com/s/ref=lp_6685289011_pg_2?rh=n%3A4991425011%2Cn%3A%214991426011%2Cn%3A6685269011%2Cn%3A6685289011&page='+str(page)+'&ie=UTF8&qid=1462681717'\n",
    "    r = requests.get(page_url)\n",
    "    r.text\n",
    "\n",
    "    b = BeautifulSoup(r.text, 'html.parser')\n",
    "\n",
    "    url_1 = []\n",
    "    for link in b.findAll('a', {'class':\"a-link-normal a-text-normal\"}):\n",
    "        url_1.append(link.get('href'))\n",
    "    \n",
    "    url_list = url_1[::2]\n",
    "\n",
    "    for url in url_list:\n",
    "        try:\n",
    "            paintings.append(scrapePainting(url))\n",
    "        except Exception as error:\n",
    "            print error\n",
    "        time.sleep(random.randint(5,20))\n",
    "    \n",
    "    if page % 10 == 0:\n",
    "        df = pd.DataFrame(paintings)\n",
    "        df.to_csv(\"output\"+str(page)+\".csv\", index=False)\n",
    "    elif page == 218:\n",
    "\n",
    "df = pd.DataFrame(paintings) \n",
    "df.to_csv(\"final_df.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "o10 = pd.read_csv('output10.csv')\n",
    "final = pd.read_csv('final_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "final.describe()\n",
    "\n",
    "#url_rej = o100[o100.isnull()]\n",
    "#url_rej.head(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Adding all the rejeected requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "paintings_rej = final_df.isnull().url_id\n",
    "paintings = []\n",
    "\n",
    "for page in range(1,219):\n",
    "    \n",
    "    page_url = 'http://www.amazon.com/s/ref=lp_6685289011_pg_3?rh=n%3A4991425011%2Cn%3A%214991426011%2Cn%3A6685269011%2Cn%3A6685289011&page='+str(page)+'&ie=UTF8&qid=1462820989'\n",
    "    r = requests.get(page_url)\n",
    "    r.text\n",
    "\n",
    "    b = BeautifulSoup(r.text, 'html.parser')\n",
    "\n",
    "    url_1 = []\n",
    "    for link in b.findAll('a', {'class':\"a-link-normal a-text-normal\"}):\n",
    "        url_1.append(link.get('href'))\n",
    "    \n",
    "    url_list = url_1[::2]\n",
    "\n",
    "    for url in url_list:\n",
    "        if url is in paintings_rej:\n",
    "            try:\n",
    "                paintings.append(scrapePainting(url))\n",
    "            except Exception as error:\n",
    "                print error\n",
    "            time.sleep(random.randint(5,20))\n",
    "    \n",
    "    if page % 50 == 0:\n",
    "        df = pd.DataFrame(paintings)\n",
    "        df.to_csv(\"rej_output\"+str(page)+\".csv\", index=False)\n",
    "    elif page == 218:\n",
    "\n",
    "df = pd.DataFrame(paintings) \n",
    "df.to_csv(\"final_rej_df.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path += ['/usr/local/Cellar/opencv/2.4.12_2']\n",
    "import numpy as np\n",
    "import urllib\n",
    "import cv2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def url_to_image(url):\n",
    "    #download the image\n",
    "    resp = urllib.urlopen(url)\n",
    "    #convert to NumPy array\n",
    "    image = np.asarray(bytearray(resp.read()), dtype ='uint8')\n",
    "    #read it into OpenCV\n",
    "    image = cv2.imdecode(image, cv2.IMREAD_COLOR)\n",
    "    \n",
    "    return image\n",
    "\n",
    "a = url_to_image('http://ecx.images-amazon.com/images/I/519lYUvAxDL._QL70_.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cv2.namesWindow(\"test\")\n",
    "cv2.imshow(\"Image\", a)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
